{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt with Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:39.428712Z",
     "start_time": "2025-03-02T13:20:39.426548Z"
    }
   },
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline,  DDIMScheduler\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.793489Z",
     "start_time": "2025-03-02T13:20:39.446272Z"
    }
   },
   "source": [
    "# MY_TOKEN = '<replace with your token>'\n",
    "MY_TOKEN = None\n",
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "\n",
    "# diffusion device\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model_key = \"runwayml/stable-diffusion-v1-5\"\n",
    "model_key = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "# load diffusion model\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(model_key, local_files_only=True, torch_dtype=torch.float16).to(device)\n",
    "# ldm_stable.enable_xformers_memory_efficient_attention()\n",
    "tokenizer = ldm_stable.tokenizer\n",
    "ldm_stable.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\",\n",
    "                                                     beta_start=0.00085,beta_end=0.012,\n",
    "                                                     steps_offset=1)\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"../blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"../blip-image-captioning-large\").to(\"cuda:0\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43d9f14b08204796adf62952572e3a6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.806248Z",
     "start_time": "2025-03-02T13:20:45.796473Z"
    }
   },
   "source": [
    "# code for store attention\n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    \n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "    \n",
    "    \n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 64 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.816796Z",
     "start_time": "2025-03-02T13:20:45.807145Z"
    }
   },
   "source": [
    "from PIL import Image\n",
    "\n",
    "# code for aggregaring attention\n",
    "def aggregate_all_attention(prompts, attention_store: AttentionStore, from_where: List[str], is_cross: bool, select: int):\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    att_8 = []\n",
    "    att_16 = []\n",
    "    att_32 = []\n",
    "    att_64 = []\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == 8*8:\n",
    "                cross_maps = item.reshape(len(prompts), -1, 8, 8, item.shape[-1])[select]\n",
    "                att_8.append(cross_maps)\n",
    "            if item.shape[1] == 16*16:\n",
    "                cross_maps = item.reshape(len(prompts), -1, 16, 16, item.shape[-1])[select]\n",
    "                att_16.append(cross_maps)\n",
    "            if item.shape[1] == 32*32:\n",
    "                cross_maps = item.reshape(len(prompts), -1, 32, 32, item.shape[-1])[select]\n",
    "                att_32.append(cross_maps)\n",
    "            if item.shape[1] == 64*64:\n",
    "                cross_maps = item.reshape(len(prompts), -1, 64, 64, item.shape[-1])[select]\n",
    "                att_64.append(cross_maps)\n",
    "    atts = []\n",
    "    for att in [att_8,att_16,att_32,att_64]:\n",
    "        att = torch.cat(att, dim=0)\n",
    "        att = att.sum(0) / att.shape[0]\n",
    "        atts.append(att.cpu())\n",
    "    return atts\n",
    "\n",
    "def aggregate_attention(prompts, attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "# visualize cross att\n",
    "def show_cross_attention(prompts,attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(prompts, attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    j = 0\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        if decoder(int(tokens[j])) == \"++\":\n",
    "            j += 1  \n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[j])))\n",
    "        images.append(image)\n",
    "        j+=1\n",
    "        if j >= len(tokens):\n",
    "            break\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "# visualize self att\n",
    "def show_self_attention_comp(prompts,attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(prompts, attention_store, res, from_where, False, select).float().numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.819862Z",
     "start_time": "2025-03-02T13:20:45.817926Z"
    }
   },
   "source": [
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None,t=NUM_DIFFUSION_STEPS):\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=t, guidance_scale=GUIDANCE_SCALE, generator=generator, low_resource=LOW_RESOURCE)\n",
    "    return images, x_t"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.828339Z",
     "start_time": "2025-03-02T13:20:45.820719Z"
    }
   },
   "source": [
    "# diffusion vae\n",
    "vae = ldm_stable.vae.to(device)\n",
    "controller = AttentionStore()\n",
    "\n",
    "def encode_imgs(imgs):\n",
    "    # imgs: [B, 3, H, W]\n",
    "    imgs = 2 * imgs - 1\n",
    "    posterior = vae.encode(imgs).latent_dist.mean\n",
    "    latents = posterior * 0.18215\n",
    "    return latents"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.831376Z",
     "start_time": "2025-03-02T13:20:45.828932Z"
    }
   },
   "source": [
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)  # 固定随机种子（CPU）\n",
    "    if torch.cuda.is_available():  # 固定随机种子（GPU)\n",
    "        torch.cuda.manual_seed(seed)  # 为当前GPU设置\n",
    "        torch.cuda.manual_seed_all(seed)  # 为所有GPU设置\n",
    "    np.random.seed(seed)  # 保证后续使用random函数时，产生固定的随机数\n",
    "    torch.backends.cudnn.benchmark = False  # GPU、网络结构固定，可设置为True\n",
    "    torch.backends.cudnn.deterministic = True  # 固定网络结构\n"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.834835Z",
     "start_time": "2025-03-02T13:20:45.832Z"
    }
   },
   "source": [
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "# cam visual_code\n",
    "def show_cam_on_image(img, mask):\n",
    "    mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=(img.size[1],img.size[0]), mode='bilinear', align_corners=False).squeeze().squeeze()\n",
    "    img = np.float32(img) / 255.\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    cam = np.uint8(255 * cam)\n",
    "    return cam\n",
    "\n",
    "# def show_attention(mask, save_path):\n",
    "#     mask = (mask-mask.min())/(mask.max()-mask.min())\n",
    "#     heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "#     attention = np.uint8(heatmap)\n",
    "#     cv2.imwrite(save_path, attention)"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.840795Z",
     "start_time": "2025-03-02T13:20:45.835413Z"
    }
   },
   "source": [
    "# 主函数\n",
    "def generate_att(\n",
    "        t, input_latent, noise, prompts, controller, pos,\n",
    "        is_self=True,\n",
    "        is_multi_self=False,\n",
    "        is_cross_norm=True,\n",
    "        weight=[0.3,0.5,0.1,0.1]\n",
    "):\n",
    "    controller.reset()\n",
    "    \n",
    "    # g_cpu = torch.Generator(4307)\n",
    "    \n",
    "    # # 根据去噪步数 t 向潜在表示表示中添加噪声\n",
    "    # latents_noisy = ldm_stable.scheduler.add_noise(\n",
    "    #     input_latent, noise, torch.tensor(t,device=device)\n",
    "    # )\n",
    "    \n",
    "    # image, x_t = run_and_display(\n",
    "    #     prompts, controller, latent=latents_noisy,run_baseline=False, generator=g_cpu,t=t\n",
    "    # )\n",
    "    \n",
    "    layers = (\"mid\", \"up\", \"down\")\n",
    "    \n",
    "    cross_attention_maps = aggregate_all_attention(prompts,controller, layers, True, 0)\n",
    "    \n",
    "    self_attention_maps = aggregate_all_attention(prompts,controller, [\"up\", \"mid\", \"down\"], False, 0)\n",
    "    \n",
    "    imgs = []\n",
    "    \n",
    "    for idx,res in enumerate([8, 16, 32, 64]):\n",
    "        out_att = cross_attention_maps[idx].permute(2,0,1).float()\n",
    "        \n",
    "        if is_cross_norm:\n",
    "            att_max = torch.amax(out_att,dim=(1,2),keepdim=True)\n",
    "            att_min = torch.amin(out_att,dim=(1,2),keepdim=True)\n",
    "            out_att = (out_att-att_min)/(att_max-att_min)\n",
    "            \n",
    "        if is_multi_self: \n",
    "            self_att = self_attention_maps[idx].view(res*res,res*res).float() \n",
    "            self_att = self_att/self_att.max()\n",
    "            out_att = torch.matmul(self_att.unsqueeze(0),out_att.view(-1,res*res,1)).view(-1,res,res)\n",
    "            \n",
    "        if res != 64:\n",
    "            out_att = F.interpolate(out_att.unsqueeze(0), size=(64,64), mode='bilinear', align_corners=False).squeeze()\n",
    "        \n",
    "        # 应用层权重并存储结果注意力图\n",
    "        imgs.append(out_att * weight[idx])\n",
    "\n",
    "    cross_att_map = torch.stack(imgs).sum(0)[pos].mean(0).view(64*64, 1)\n",
    "\n",
    "    if is_self and not is_multi_self:\n",
    "        self_att = self_attention_maps[3].view(64*64,64*64).float() \n",
    "        self_att = self_att/self_att.max()\n",
    "        for i in range(1):\n",
    "            cross_att_map = torch.matmul(self_att,cross_att_map)\n",
    "            \n",
    "    att_map = cross_att_map.view(res,res)\n",
    "    att_map = F.interpolate(att_map.unsqueeze(0).unsqueeze(0), size=(512,512), mode='bilinear', align_corners=False).squeeze().squeeze()\n",
    "    \n",
    "    # 归一化并使用Sigmoid增强对比度\n",
    "    att_map = (att_map-att_map.min())/(att_map.max()-att_map.min())\n",
    "    att_map = F.sigmoid(8 * (att_map-0.4))\n",
    "    att_map = (att_map-att_map.min())/(att_map.max()-att_map.min()) \n",
    "\n",
    "    \"\"\"\n",
    "    att_map_map = Image.fromarray((att_map.cpu().detach().numpy()*255).astype(np.uint8),mode=\"L\")\n",
    "    \n",
    "    display(att_map_map)\n",
    "    \n",
    "    print(\"8x8 cross att map\")\n",
    "    show_cross_attention(prompts,controller, res=8, from_where=layers)\n",
    "    print(\"8x8 self att map\")\n",
    "    show_self_attention_comp(prompts,controller, res=8, from_where=layers)\n",
    "    \n",
    "    print(\"16x16 cross att map\")\n",
    "    show_cross_attention(prompts,controller, res=16, from_where=layers)\n",
    "    print(\"16x16 self att map\")\n",
    "    show_self_attention_comp(prompts,controller, res=16, from_where=layers)\n",
    "    \n",
    "    print(\"32x32 cross att map\")\n",
    "    show_cross_attention(prompts,controller, res=32, from_where=layers)\n",
    "    print(\"32x32 self att map\")\n",
    "    show_self_attention_comp(prompts,controller, res=32, from_where=layers)\n",
    "    \n",
    "    print(\"64x64 cross att map\")\n",
    "    show_cross_attention(prompts,controller, res=64, from_where=layers)\n",
    "    print(\"64x64 self att map\")\n",
    "    show_self_attention_comp(prompts,controller, res=64, from_where=layers)\n",
    "\n",
    "    # print(\"64x64 self att map\")\n",
    "    # show_self_attention_comp(prompts,controller, res=64, from_where=layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    return att_map"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:45.843361Z",
     "start_time": "2025-03-02T13:20:45.841400Z"
    }
   },
   "source": [
    "# from torchvision import transforms\n",
    "# from IPython.display import display\n",
    "# from PIL import Image\n",
    "# with torch.no_grad():\n",
    "#     same_seeds(3407)\n",
    "#     \n",
    "#     img_path = \"./sample_img/thread.png\"\n",
    "#     input_img = Image.open(img_path).convert(\"RGB\")\n",
    "# \n",
    "#     print(\"ori_image\")\n",
    "#     display(input_img)\n",
    "#     \n",
    "#     # 图像转换操作\n",
    "#     t = []\n",
    "#     t.append(transforms.ToTensor())\n",
    "#     transforms = transforms.Compose(t)\n",
    "# \n",
    "#     img_tensor = (transforms(input_img).unsqueeze(0)).to(device)\n",
    "# \n",
    "#     rgb_512 = F.interpolate(img_tensor, (512, 512), mode='bilinear', align_corners=False).half()\n",
    "# \n",
    "#     input_latent = encode_imgs(rgb_512)\n",
    "# \n",
    "#     noise = torch.randn_like(input_latent).to(device)\n",
    "# \n",
    "#     raw_image = input_img\n",
    "# \n",
    "#     # 目标类名称\n",
    "#     cls_name = \"thread\"\n",
    "#     text = f\"a photograph of {cls_name}\"\n",
    "#     \n",
    "#     # 使用BLIP进行文本-图像输入处理\n",
    "#     inputs = processor(raw_image,text,return_tensors=\"pt\").to(\"cuda\")  \n",
    "# \n",
    "#     # 使用BLIP生成新的文本描述，进行增强\n",
    "#     # use blip and \"++\" emphasizing semantic information of target categories\n",
    "#     out = model.generate(**inputs)\n",
    "#     texts = processor.decode(out[0], skip_special_tokens=True)\n",
    "#     texts = text +\"++\"+ texts[len(text):]       # 增强目标类别描述，加入更多语义信息\n",
    "# \n",
    "#     g_cpu = torch.Generator(3407)\n",
    "#     prompts = [texts]\n",
    "#     print(\"**** blip_prompt: \"+texts+\"****\")    # 打印生成的增强文本\n",
    "# \n",
    "#     # 以下参数设置：  \n",
    "#     # - `pos`是目标类别在句子中的位置，例如\"plane\"在\"a photograph of plane\"中的位置是4\n",
    "#     # - `t`是去噪步数，通常在50到150之间\n",
    "#     pos = [4]   \n",
    "#     t = 100\n",
    "#     \n",
    "#     # 生成注意力掩码，用于引导生成模型\n",
    "#     mask = generate_att(\n",
    "#         t,input_latent, noise, prompts, controller, pos,\n",
    "#         is_self=True,\n",
    "#         is_multi_self=False,\n",
    "#         is_cross_norm=True,\n",
    "#         weight=[0.3, 0.5, 0.1, 0.1]\n",
    "#     )\n",
    "# \n",
    "#     cam = show_cam_on_image(raw_image, mask)\n",
    "#     print(\"visual_cam\")\n",
    "#     display(Image.fromarray(cam[:,:,::-1]))"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:57.617398Z",
     "start_time": "2025-03-02T13:20:45.844286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "baseline：计算MIoU与推理速度\n",
    "\"\"\"\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "\n",
    "def calculate_iou(gt_mask, pred_mask):\n",
    "    \"\"\"\n",
    "    计算 IoU（Intersection over Union）\n",
    "    :param gt_mask: Ground Truth Mask (numpy array)\n",
    "    :param pred_mask: Predicted Mask (numpy array)\n",
    "    :return: IoU score\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "    union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "    if union == 0:\n",
    "        return 0  # 避免除零错误\n",
    "    \n",
    "    iou = intersection / union\n",
    "    print(f'iou:{iou:.2f}')\n",
    "    return iou\n",
    "\n",
    "def calculate_miou(gt_folder, pred_folder):\n",
    "    \"\"\"\n",
    "    计算文件夹中所有图片的 MIoU\n",
    "    :param gt_folder: Ground Truth 图片文件夹路径\n",
    "    :param pred_folder: 预测 Mask 图片文件夹路径\n",
    "    :return: MIoU 值\n",
    "    \"\"\"\n",
    "    gt_files = sorted(os.listdir(gt_folder))\n",
    "    pred_files = sorted(os.listdir(pred_folder))\n",
    "    \n",
    "    if len(gt_files) != len(pred_files):\n",
    "        raise ValueError(\"Ground Truth 和预测 Mask 的图片数量不一致！\")\n",
    "    \n",
    "    iou_scores = []\n",
    "    \n",
    "    for gt_file, pred_file in zip(gt_files, pred_files):\n",
    "        # 加载 Ground Truth 和预测 Mask\n",
    "        gt_path = os.path.join(gt_folder, gt_file)\n",
    "        pred_path = os.path.join(pred_folder, pred_file)\n",
    "        \n",
    "        gt_mask = np.array(Image.open(gt_path).convert(\"L\"))  # 转换为灰度图\n",
    "        pred_mask = np.array(Image.open(pred_path).convert(\"L\"))  # 转换为灰度图\n",
    "        \n",
    "        # 二值化处理（假设 Ground Truth 和预测 Mask 是二值图像）\n",
    "        gt_mask = (gt_mask > 128).astype(np.uint8)  # 阈值化\n",
    "        pred_mask = (pred_mask > 128).astype(np.uint8)  # 阈值化\n",
    "        \n",
    "        # 计算 IoU\n",
    "        iou = calculate_iou(gt_mask, pred_mask)\n",
    "        iou_scores.append(iou)\n",
    "    \n",
    "    # 计算 MIoU\n",
    "    miou = np.mean(iou_scores)\n",
    "    return miou\n",
    "\n",
    "\n",
    "def baseline(img_path, pred_folder, cls_name):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        same_seeds(3407)\n",
    "        \n",
    "        # 加载并预处理图像\n",
    "        input_img = Image.open(img_path).convert(\"RGB\")\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        img_tensor = transform(input_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 调整尺寸并编码到前在空间\n",
    "        rgb_512 = F.interpolate(img_tensor, (512, 512), mode='bilinear', align_corners=False).half()\n",
    "        input_latent = encode_imgs(rgb_512)\n",
    "        noise = torch.randn_like(input_latent).to(device)\n",
    "    \n",
    "        # 生成增强文本描述\n",
    "        text = f\"a photograph of {cls_name}\" \n",
    "        \n",
    "        # 使用BLIP进行文本-图像输入处理\n",
    "        inputs = processor(input_img, text,return_tensors=\"pt\").to(\"cuda\")  \n",
    "        # 使用BLIP生成新的文本描述，进行增强\n",
    "        # use blip and \"++\" emphasizing semantic information of target categories\n",
    "        out = model.generate(**inputs)\n",
    "        texts = processor.decode(out[0], skip_special_tokens=True)\n",
    "        texts = text +\"++\"+ texts[len(text):]       # 增强目标类别描述，加入更多语义信息\n",
    "        \n",
    "        prompts = [texts]\n",
    "    \n",
    "        # 以下参数设置：  \n",
    "        # - `pos`是目标类别在句子中的位置，例如\"plane\"在\"a photograph of plane\"中的位置是4\n",
    "        # - `t`是去噪步数，通常在50到150之间\n",
    "        pos = [4]   \n",
    "        t = 10\n",
    "        \n",
    "        # 生成注意力掩码，用于引导生成模型\n",
    "        mask = generate_att(\n",
    "            t,input_latent, noise, prompts, controller, pos,\n",
    "            is_self=True,\n",
    "            is_multi_self=False,\n",
    "            is_cross_norm=True,\n",
    "            weight=[0.1, 0.4, 0.4, 0.1]\n",
    "        )\n",
    "        \n",
    "        pred_mask_np = mask.cpu().numpy()\n",
    "        \n",
    "        threshold = 0.5  # 推荐范围 [0.3, 0.7]\n",
    "        binary_mask = (pred_mask_np > threshold).astype(np.uint8) * 255  # 二值化为0或255\n",
    "        \n",
    "        # 步骤3：转换为PIL图像并调整尺寸\n",
    "        pred_mask = Image.fromarray(binary_mask).convert(\"L\")\n",
    "        \n",
    "        # 步骤4：保持与原图相同尺寸（重要！确保与GT对齐）\n",
    "        original_size = Image.open(img_path).size  # 获取原始图像尺寸\n",
    "        pred_mask = pred_mask.resize(original_size)  # 调整到原始尺寸\n",
    "        \n",
    "        # 保存结果（保持与原始文件名一致）\n",
    "        if not os.path.exists(pred_folder):\n",
    "            os.makedirs(pred_folder)\n",
    "        pred_path = os.path.join(pred_folder, os.path.basename(img_path))\n",
    "        pred_mask.save(pred_path)\n",
    "        \n",
    "    return time.time() - start_time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class_name = 'scratch'\n",
    "    \n",
    "    img_folder = f'/home/saki/data/mvtec_anomaly_detection/wood/test/{class_name}'\n",
    "    gt_folder = f'/home/saki/data/mvtec_anomaly_detection/wood/ground_truth/{class_name}'\n",
    "    pred_folder = f\"/home/saki/data/mvtec_anomaly_detection/wood/predictions/{class_name}\"\n",
    "\n",
    "    # 遍历所有测试图像\n",
    "    total_time = 0\n",
    "    img_paths = sorted([os.path.join(img_folder, f) for f in os.listdir(img_folder)])\n",
    "    for idx, img_path in enumerate(img_paths):\n",
    "        # cls_name = class_names  # 根据实际情况获取类别\n",
    "        inference_time = baseline(\n",
    "            img_path, pred_folder, class_name\n",
    "        )\n",
    "        total_time += inference_time\n",
    "        print(f\"Processed {idx+1}/{len(img_paths)}, Time: {inference_time:.2f}s\")\n",
    "    \n",
    "    # 性能评估\n",
    "    miou = calculate_miou(gt_folder, pred_folder)\n",
    "    avg_speed = total_time / len(img_paths)\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"MIoU: {miou:.4f}\")\n",
    "    print(f\"Average Speed: {avg_speed:.2f} seconds/image\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/21, Time: 0.59s\n",
      "Processed 2/21, Time: 0.53s\n",
      "Processed 3/21, Time: 0.59s\n",
      "Processed 4/21, Time: 0.54s\n",
      "Processed 5/21, Time: 0.54s\n",
      "Processed 6/21, Time: 0.60s\n",
      "Processed 7/21, Time: 0.58s\n",
      "Processed 8/21, Time: 0.53s\n",
      "Processed 9/21, Time: 0.53s\n",
      "Processed 10/21, Time: 0.53s\n",
      "Processed 11/21, Time: 0.59s\n",
      "Processed 12/21, Time: 0.54s\n",
      "Processed 13/21, Time: 0.54s\n",
      "Processed 14/21, Time: 0.53s\n",
      "Processed 15/21, Time: 0.54s\n",
      "Processed 16/21, Time: 0.58s\n",
      "Processed 17/21, Time: 0.54s\n",
      "Processed 18/21, Time: 0.55s\n",
      "Processed 19/21, Time: 0.54s\n",
      "Processed 20/21, Time: 0.60s\n",
      "Processed 21/21, Time: 0.54s\n",
      "iou:0.54\n",
      "iou:0.25\n",
      "iou:0.72\n",
      "iou:0.61\n",
      "iou:0.47\n",
      "iou:0.80\n",
      "iou:0.63\n",
      "iou:0.80\n",
      "iou:0.59\n",
      "iou:0.69\n",
      "iou:0.52\n",
      "iou:0.28\n",
      "iou:0.70\n",
      "iou:0.62\n",
      "iou:0.78\n",
      "iou:0.55\n",
      "iou:0.36\n",
      "iou:0.40\n",
      "iou:0.48\n",
      "iou:0.29\n",
      "iou:0.53\n",
      "\n",
      "Evaluation Results:\n",
      "MIoU: 0.5538\n",
      "Average Speed: 0.55 seconds/image\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T13:20:57.619551Z",
     "start_time": "2025-03-02T13:20:57.618091Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 58
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
